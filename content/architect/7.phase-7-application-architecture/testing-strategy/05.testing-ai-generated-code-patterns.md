---
title: "Testing AI-Generated Code Patterns"
description: "Learn practical patterns for verifying AI-generated code, from snapshot testing to mutation testing and adversarial inputs."
estimatedMinutes: 15
difficulty: intermediate
learningObjectives:
  - "Understand why AI-generated code needs different testing approaches"
  - "Apply snapshot testing to detect unexpected changes in AI output"
  - "Use mutation testing to verify that tests actually catch bugs"
  - "Design adversarial test inputs that expose common AI code weaknesses"
  - "Build a quality gate checklist for AI-generated code in CI/CD"
prerequisites:
  - "tdd-with-ai-agents"
quiz:
  passingScore: 70
  questions:
    - question: "Why does AI-generated code need extra testing beyond standard practices?"
      type: single
      options:
        - "AI can produce code that looks correct but has subtle logic errors, hallucinated APIs, or missing edge cases"
        - "AI-generated code always contains security vulnerabilities"
        - "Standard testing tools cannot run against AI-generated code"
        - "AI code uses a different programming language"
      correctAnswer: "AI can produce code that looks correct but has subtle logic errors, hallucinated APIs, or missing edge cases"
      explanation: "AI-generated code often looks clean and well-structured, which makes it easy to trust. But it can contain subtle bugs: functions that do not exist, libraries at wrong versions, missing null checks, or logic that works for common cases but fails on edge cases. Extra testing patterns help catch these issues."

    - question: "What does mutation testing measure?"
      type: single
      options:
        - "Whether your tests can detect small changes (mutations) in the code"
        - "How fast your code mutates during execution"
        - "The number of lines changed by the AI"
        - "The DNA of the codebase"
      correctAnswer: "Whether your tests can detect small changes (mutations) in the code"
      explanation: "Mutation testing introduces small changes to the code (like changing > to >= or removing a line). If your tests still pass after a mutation, those tests are weak -- they did not catch the change. A high mutation score means your tests are good at catching real bugs."

    - question: "Select all common weaknesses in AI-generated code that tests should target."
      type: multiple
      options:
        - "Missing null and empty input checks"
        - "Using APIs or functions that do not exist"
        - "Perfect handling of all edge cases"
        - "Incorrect error handling patterns"
      correctAnswers:
        - "Missing null and empty input checks"
        - "Using APIs or functions that do not exist"
        - "Incorrect error handling patterns"
      explanation: "AI-generated code commonly misses null/empty checks, references APIs that were hallucinated or deprecated, and uses incorrect error handling. 'Perfect handling of all edge cases' is not a weakness -- it is the opposite. Your tests should specifically target these known weakness patterns."

    - question: "Snapshot testing captures the output of AI-generated code and alerts you when the output changes."
      type: true-false
      correctAnswer: true
      explanation: "Snapshot testing records the output of a function or component. On future runs, it compares the current output to the saved snapshot. If the AI regenerates code and the output changes, the snapshot test fails and forces you to review whether the change is correct or a regression."

    - question: "What is the purpose of adversarial testing for AI-generated code?"
      type: single
      options:
        - "To deliberately send tricky inputs that expose hidden bugs and assumptions"
        - "To make the AI generate code faster"
        - "To adversarially compete with other AI models"
        - "To test the AI model's training data"
      correctAnswer: "To deliberately send tricky inputs that expose hidden bugs and assumptions"
      explanation: "Adversarial testing sends inputs that are designed to break assumptions: empty strings, null values, extremely large numbers, special characters, and boundary conditions. AI-generated code often handles the 'happy path' well but fails on these tricky inputs."

    - question: "What should a quality gate for AI-generated code check before allowing a merge?"
      type: single
      options:
        - "Tests pass, no hallucinated imports, lint clean, and mutation score above threshold"
        - "The AI model version number"
        - "Only that the code compiles"
        - "The number of lines generated"
      correctAnswer: "Tests pass, no hallucinated imports, lint clean, and mutation score above threshold"
      explanation: "A proper quality gate checks multiple dimensions: all tests must pass, static analysis catches hallucinated or deprecated imports, linting ensures code style, and mutation testing confirms the tests are meaningful. Just compiling is not enough -- the code must be correct and well-tested."
---

# Testing AI-Generated Code Patterns

AI coding agents can produce hundreds of lines of code in seconds. The code looks clean, is well-commented, and follows common patterns. But looking correct is not the same as being correct.

AI-generated code has specific failure patterns: it may call functions that do not exist (hallucination), use outdated APIs, miss null checks, or handle common cases perfectly while failing on edge cases. Standard testing catches some of these issues. But targeted testing patterns catch more.

This lesson covers practical patterns for testing AI-generated code so you can trust what your AI agents produce.

## Why AI Code Needs Special Attention

When a human developer writes code, they understand the system context. They know which libraries are installed, which API version they use, and which edge cases matter for the business. AI agents do not have this full context.

Common AI code weaknesses include:

| Weakness | Example | Risk |
|----------|---------|------|
| **Hallucinated APIs** | Calling `response.getData()` when the real method is `response.data` | Runtime crash |
| **Outdated patterns** | Using a deprecated library or removed function | Future breakage |
| **Missing null checks** | Assuming input is always present | Production error |
| **Happy path only** | Works for "Alice" but fails for empty string | Edge case bug |
| **Wrong error handling** | Catching all exceptions silently | Hidden failures |
| **Subtle logic errors** | Off-by-one, wrong comparison operator | Incorrect results |

## Pattern 1: Snapshot Testing

Snapshot testing captures the output of a function and saves it. On later runs, it compares the current output to the saved snapshot. If the output changes, the test fails.

This is useful when AI regenerates code. You want to know if the new version produces different results.

```javascript
// Snapshot test for a formatting function
test('formatInvoice produces expected output', () => {
  const invoice = { id: 1, amount: 99.99, customer: 'Alice' };
  const result = formatInvoice(invoice);
  expect(result).toMatchSnapshot();
});
```

The first run saves the snapshot. If the AI later changes `formatInvoice`, the snapshot test catches any difference in output. You then review whether the change is intentional or a regression.

**When to use:** Functions with complex output (formatted text, HTML, JSON structures, reports).

## Pattern 2: Mutation Testing

Mutation testing answers the question: "Are my tests actually good enough to catch real bugs?"

It works by making small changes (mutations) to the code:
- Changing `>` to `>=`
- Replacing `true` with `false`
- Removing a function call
- Changing a `+` to a `-`

After each mutation, it runs your test suite. If the tests still pass, the mutation "survived" -- meaning your tests are too weak to catch that type of bug.

```bash
# Example: Running mutation testing with Stryker (JavaScript)
npx stryker run

# Output:
# Mutant survived: Changed > to >= on line 42
# Mutant killed: Changed + to - on line 15
# Mutation score: 78%
```

A mutation score of 80% or higher means your tests catch 80% of small code changes. This is especially important for AI-generated code, where the test suite might have been partly generated by the same AI.

**When to use:** After the AI writes both code and tests. Mutation testing reveals whether those tests are actually meaningful.

## Pattern 3: Adversarial Input Testing

Adversarial testing deliberately sends inputs designed to break assumptions. AI-generated code often handles common inputs well but fails on boundary cases.

### A Standard Adversarial Input Set

```javascript
const adversarialInputs = [
  // Empty and null
  null, undefined, '', ' ', '   ',
  // Boundary numbers
  0, -1, -0, Infinity, -Infinity, NaN,
  // Large values
  'a'.repeat(1_000_000),    // Very long string
  Number.MAX_SAFE_INTEGER,  // Very large number
  // Special characters
  "<script>alert('xss')</script>",
  "'; DROP TABLE users; --",
  // Unicode
  '\0',       // Null byte
  '\u{1F600}', // Emoji
  '\u200B',   // Zero-width space
  // Type mismatches (for dynamic languages)
  [], {}, true, 42.0,
];
```

Run each of these through every function the AI generates. The function should either handle the input correctly or raise a clear, expected error. It should never crash with an unhandled exception.

```javascript
test('function handles adversarial inputs', () => {
  for (const input of adversarialInputs) {
    try {
      const result = aiGeneratedFunction(input);
      // If it returns, the result should be valid
      if (input !== null && input !== undefined) {
        expect(result).toBeDefined();
      }
    } catch (error) {
      // Expected errors (TypeError, RangeError) are fine
      if (!(error instanceof TypeError) && !(error instanceof RangeError)) {
        throw new Error(`Unexpected error for input ${JSON.stringify(input)}: ${error}`);
      }
    }
  }
});
```

## Pattern 4: Import and Dependency Verification

AI agents sometimes reference libraries, functions, or methods that do not exist. A simple static check catches these before runtime.

```javascript
// Check that all imports in AI-generated code actually resolve
test('all imports resolve', () => {
  const modulesUsed = ['express', 'lodash', 'dayjs', 'zod'];
  for (const moduleName of modulesUsed) {
    try {
      require.resolve(moduleName);
    } catch {
      throw new Error(`AI used non-existent module: ${moduleName}`);
    }
  }
});
```

In CI/CD, you can also run linting tools that flag undefined names, unused imports, and deprecated function calls.

## Building a Quality Gate

Combine these patterns into a quality gate that runs in your CI/CD pipeline whenever AI-generated code is merged.

::illustration-checklist
---
title: AI Code Quality Gate
items:
  - text: All unit and integration tests pass
    icon: "\U00002705"
  - text: No hallucinated or missing imports
    icon: "\U0001F4E6"
  - text: Adversarial input tests pass
    icon: "\U0001F6E1\uFE0F"
  - text: Mutation score above 75%
    icon: "\U0001F9EC"
  - text: Linting and static analysis clean
    icon: "\U0001F9F9"
  - text: Human review of all assertions and error handling
    icon: "\U0001F440"
note: Block merge if any check fails
color: amber
size: 2xl
---
::

## Best Practices

- **Run adversarial inputs on every new function** the AI generates. Keep a standard set of tricky inputs and expand it over time.
- **Use mutation testing to audit AI-generated tests** -- If the AI wrote both code and tests, mutation testing tells you whether the tests are real or superficial.
- **Save snapshots for complex outputs** so you can detect regressions when the AI regenerates code.
- **Automate import checking** in your linter configuration. Flag any module or method that does not exist.
- **Track AI code quality metrics** over time: mutation scores, adversarial test pass rates, and defect rates.

## Common Mistakes to Avoid

1. **Trusting AI-generated tests without review** -- AI-generated tests often test the happy path and miss edge cases. They may even contain assertions that are always true. Always review the logic of test assertions manually.

2. **Only running happy-path tests** -- If you only test with "normal" inputs like "Alice" and 42, you miss the bugs that appear with empty strings, negative numbers, and null values. Adversarial testing fills this gap.

## Key Takeaways

- AI-generated code has specific weaknesses: hallucinated APIs, missing null checks, happy-path-only logic
- Snapshot testing catches output regressions when AI regenerates code
- Mutation testing reveals whether tests are strong enough to catch real bugs
- Adversarial input testing deliberately targets boundary conditions AI often misses
- Import verification catches hallucinated libraries and deprecated functions
- Combine all patterns into a CI/CD quality gate that blocks weak code from merging
